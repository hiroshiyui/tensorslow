{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba13450f-c22f-438f-b05a-70f9abfb7bea",
   "metadata": {
    "editable": true,
    "execution": {
     "execution_failed": "2025-02-26T02:05:45.434Z",
     "iopub.execute_input": "2025-02-26T02:02:40.314559Z",
     "iopub.status.busy": "2025-02-26T02:02:40.314340Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2025-02-26 10:03:06.386065: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df29458142427e8af30f74de12a1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import torch\n",
    "from mtkresearch.llm.prompt import MRPromptV3\n",
    "\n",
    "model_id = 'MediaTek-Research/Llama-Breeze2-8B-Instruct-v0_1'\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    img_context_token_id=128212\n",
    ").eval()\n",
    "\n",
    "model = AutoModel.from_pretrained(\"mradermacher/Llama-Breeze2-8B-Instruct-text-only-GGUF\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "  max_new_tokens=2048,\n",
    "  do_sample=True,\n",
    "  temperature=0.01,\n",
    "  top_p=0.01,\n",
    "  repetition_penalty=1.1,\n",
    "  eos_token_id=128009\n",
    ")\n",
    "\n",
    "prompt_engine = MRPromptV3()\n",
    "\n",
    "sys_prompt = 'You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.'\n",
    "\n",
    "def _inference(tokenizer, model, generation_config, prompt, pixel_values=None):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    if pixel_values is None:\n",
    "        output_tensors = model.generate(**inputs, generation_config=generation_config)\n",
    "    else:\n",
    "        output_tensors = model.generate(**inputs, generation_config=generation_config, pixel_values=pixel_values.to(model.dtype))\n",
    "    output_str = tokenizer.decode(output_tensors[0])\n",
    "    return output_str\n",
    "\n",
    "conversations = [\n",
    "    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"請問什麼是深度學習？\"},\n",
    "]\n",
    "\n",
    "prompt = prompt_engine.get_prompt(conversations)\n",
    "output_str = _inference(tokenizer, model, generation_config, prompt)\n",
    "result = prompt_engine.parse_generated_str(output_str)\n",
    "print(result)\n",
    "# {'role': 'assistant', 'content': '深度學習是一種人工智慧技術，主要是透過模仿生物神經網路的結構和功能來實現。它利用大量數據進行訓練，以建立複雜的模型並使其能夠自主學習、預測或分類輸入資料。\\n\\n在深度學習中，通常使用多層的神經網路，每一層都包含許多相互連接的節點（稱為神經元）。這些神經元可以處理不同特徵的輸入資料，並將結果傳遞給下一層的神經元。隨著資料流向更高層次，這個過程逐漸捕捉到更抽象的概念或模式。\\n\\n深度學習已被廣泛應用於各種領域，如圖像識別、自然語言處理、語音識別以及遊戲等。它提供了比傳統機器學習方法更好的表現，因為它能夠從複雜且非線性的數據中提取出有用的資訊。'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913183d6-053a-41da-8b46-847c8137d7be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
